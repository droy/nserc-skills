---
name: nserc-discovery-grant
description: This skill should be used when writing or reviewing NSERC Discovery Grant applications - provides Merit Indicator framework, cross-disciplinary writing guidance, and systematic coverage checks for the three evaluation criteria (Excellence of Researcher, Merit of Proposal, HQP Training)
---

# NSERC Discovery Grant Writing

## Overview

This skill guides the preparation of NSERC Discovery Grant applications by grounding all advice in **Merit Indicators** - the specific items that reviewers evaluate using a 6-level scale (Exceptional, Outstanding, Very Strong, Strong, Moderate, Insufficient).

**When to use this skill**:
- Drafting any section of a Discovery Grant application
- Reviewing application drafts for completeness
- Planning application strategy and timeline
- Addressing reviewer feedback from previous submissions

**Key principles**:
- **Package coherence**: NSERC applications are evaluated as an integrated package. All components (research proposal, CCV, HQP training plan, budget justification) must tell a coherent story that addresses the three evaluation criteria.
- **Narrative prose**: Application.md text sections should be written in flowing paragraphs, not bullet points or heavily formatted Markdown. Reviewers read continuous prose, not slide decks.

---

## NSERC Evaluation Framework

### Three Evaluation Criteria

**Official Merit Indicators document**: https://www.nserc-crsng.gc.ca/_doc/Professors-Professeurs/DG_Merit_Indicators_eng.pdf

NSERC reviewers evaluate applications based on three criteria, each assessed independently:

1. **Excellence of the Researcher**
   - Contributions to research
   - Contributions to training of HQP
   - Contributions to other activities (knowledge mobilization, EDI, etc.)

2. **Merit of the Proposal**
   - Originality and significance of research
   - Feasibility and appropriateness of approach
   - Clarity and quality of proposal presentation

3. **Training of Highly Qualified Personnel (HQP)**
   - Quality and impact of proposed HQP training
   - Appropriateness of training environment and opportunities

### The 6-Level Rating Scale

Each Merit Indicator item is rated on this scale:
- **Exceptional** - Among the very best (top ~5%)
- **Outstanding** - Excellent, well above average
- **Very Strong** - Above average, clearly good
- **Strong** - Meets expectations, solid work
- **Moderate** - Below expectations, concerns exist
- **Insufficient** - Does not meet requirements

**Critical insight**: Reviewers don't compare your application to an ideal - they compare it to other applications in the competition. Your job is to make it easy for reviewers to place you in the top categories.

---

## Top 10 Tips (Integrated)

These tips come from experienced NSERC reviewers and should guide every aspect of your application:

**#1: Review grants from senior successful colleagues**
- Ask senior faculty in your department if you can read their successful applications
- Observe how they structure proposals, describe significance, present timelines

**#2: Write it as a package, not separate parts**
- Ensure research proposal connects to past contributions (CCV)
- Verify HQP training plan aligns with proposed research activities
- Check that budget justification matches proposed work
- Tell one coherent story across all components

**#3: Don't sweat the budget if you're new**
- For early-career applicants, modest budgets are fine
- Reviewers care more about research excellence than budget size
- Justify what you request clearly, but don't inflate

**#4: Don't be too ambitious**
- Better to propose focused, feasible work than overreach
- Reviewers value realistic timelines over ambitious scope
- Show you understand potential challenges

**#5: Writing style counts (but don't obsess)**
- Clear, accessible writing > formal academic prose
- Short sentences, active voice, logical flow
- One typo won't kill your application, but sloppy writing signals carelessness

**#6: Write for a GENERAL audience**
- **THIS IS CRITICAL**: Your committee includes reviewers outside your subfield
- Avoid jargon; explain technical concepts in accessible terms
- Start with motivation before technical details
- A non-specialist should understand why your work matters
- **Note**: This applies to ALL applicants - early career, mid-career, and senior researchers alike. Even the most accomplished researchers must make their NSERC proposals accessible to cross-disciplinary committees.

**#7: Ensure you've read the most competitive applications you can get your hands on**
- See Tip #1 - reading successful grants is the best teacher
- Notice how top applicants frame significance, feasibility, training

**#8: Ensure you're addressing the Merit Indicators for each section**
- This is non-negotiable: systematically address every relevant Merit Indicator
- Use the Merit Indicator coverage checklist (see below)
- Don't assume reviewers will infer - make connections explicit

**#9: Address ALL items in the Merit Indicators, even when not filling a major paragraph**
- Brief mentions count - you don't need full paragraphs for every item
- But complete absence of an item signals a gap
- Coverage > depth for less central items

**#10: CAREFULLY read the instructions in the online application**
- Official instructions: https://www.nserc-crsng.gc.ca/ResearchPortal-PortailDeRecherche/Instructions-Instructions/DG-SD_eng.asp
- Page limits, formatting requirements, required sections
- What goes in CCV vs application form
- Deadline dates and submission process
- Instructions change - verify you have current version

---

## Application Components and Structure

### 1. Research Proposal (5 pages max)

**Purpose**: Address Merit Indicators under "Merit of the Proposal" criterion

**Required elements**:
- **Title**: Clear, accessible to general audience
- **Background and context**: Why does this research matter?
- **Objectives**: Specific, feasible goals for grant period (typically 5 years)
- **Methodology**: How will you achieve objectives? Why is this approach appropriate?
- **Significance**: What will we know/be able to do that we can't now?
- **Feasibility**: Why can YOU do this? What makes it realistic?
- **Timeline**: Rough breakdown of activities across grant period

**Key Merit Indicators to address**:
- Originality and significance of research proposal
- Feasibility and appropriateness of the proposed approach
- Clarity and quality of the proposal presentation

**Critical writing guidance**:
- **Lead with motivation** (Tip #6): Non-specialists must understand why this matters
- **Define terms**: Don't assume shared vocabulary across subfields
- **Show, don't tell**: Instead of "This is significant," explain the impact
- **Connect to past work**: Reference your CCV to show you're prepared (Tip #2)

**Common pitfall**: Writing for specialists in your subfield. NSERC committees are cross-disciplinary. If a reviewer outside your area can't understand your proposal's significance, you lose points.

### 2. Common CV (CCV) - Past Contributions

**Purpose**: Address Merit Indicators under "Excellence of the Researcher" criterion

**What it demonstrates**:
- Research contributions (publications, presentations, invited talks)
- HQP training record (students supervised, their outcomes)
- Other contributions (reviewing, editing, knowledge mobilization, EDI activities)

**How it connects to proposal**:
- Shows you have expertise to do proposed research
- Demonstrates track record of productivity
- Proves you can train HQP successfully

**Key strategy** (Tip #2):
- Ensure CCV aligns with proposed research direction
- If proposing new direction, explain how past work provides foundation
- Highlight students' success to support HQP training plan

### 3. HQP Training Plan

**Purpose**: Address Merit Indicators under "Training of HQP" criterion

**Required elements**:
- **Current and recent HQP**: Number of students, their levels (MSc, PhD, postdoc)
- **Training activities**: Regular meetings, seminars, workshops, presentation opportunities
- **Skill development**: Technical skills and professional skills (writing, presenting, collaboration)
- **Supervision record**: Completion rates, time-to-degree, career outcomes
- **Training environment**: Lab resources, departmental support, collaborative opportunities
- **Connection to research**: How will HQP contribute to proposed research?
- **EDI considerations**: Inclusive training environment, support for diverse trainees

**Key Merit Indicators to address**:
- Quality and impact of proposed HQP training
- Appropriateness of training environment and opportunities
- (From Excellence criterion) Contributions to training of HQP - demonstrated in CCV

**Common pitfall**: Treating HQP section as afterthought. This is one of three evaluation criteria - give it serious attention.

**What "Exceptional" looks like**:
- Strong supervision track record (timely completions, student success)
- Clear training philosophy and concrete activities
- Students go on to successful careers (academia, industry, government)
- Inclusive, supportive training environment
- Integration of training with research objectives

### 4. Budget Justification

**Purpose**: Explain how you'll use requested funds

**What to include**:
- **Explanation of major expenses**: Why do you need this equipment, travel, student support?
- **Connection to proposal**: How does each expense enable proposed research?
- **Realistic estimates**: Based on current prices, institutional policies

**Key strategy** (Tip #3):
- Justify clearly but don't inflate
- For early-career applicants, modest budgets are fine
- Reviewers check that budget aligns with proposed activities (Tip #2)

**Common pitfall**: Requesting expensive items without clear justification of necessity

---

## Review Workflow: Using This Skill

### When Reviewing a Draft Section

**Step 1: Identify which evaluation criterion this section addresses**
- Research proposal → Merit of the Proposal
- CCV → Excellence of the Researcher
- HQP training plan → Training of HQP

**Step 2: Map content to specific Merit Indicator items**
- List the items from that criterion
- Check which are addressed in the draft
- Identify gaps

**Step 3: Assess from reviewer perspective**
- Put on "reviewer's hat": Cross-disciplinary committee member
- Ask: "If I'm outside this subfield, can I understand the significance?"
- Ask: "Where would I rate this on the 6-level scale?"

**Step 4: Check cross-component consistency** (Tip #2)
- Does research proposal align with CCV expertise?
- Does HQP training plan connect to research objectives?
- Does budget justification match proposed activities?

**Step 5: Apply Top 10 Tips**
- Tip #6 especially: Is this accessible to general audience?
- Tip #8: Are ALL relevant Merit Indicators addressed?
- Tip #9: Even brief mentions for less central items?

**Step 6: Provide structured feedback**
- Reference specific Merit Indicator items
- Explain what reviewers will be looking for
- Suggest concrete improvements
- Note gaps in coverage

### When Providing Feedback

**Always**:
- ✅ Reference Merit Indicators by name
- ✅ Explain reviewer perspective
- ✅ Use 6-level scale language ("For Outstanding rating, you'd need...")
- ✅ Check cross-component consistency
- ✅ Cite relevant Top 10 Tips by number
- ✅ Emphasize writing for general audience (Tip #6)
- ✅ Provide Merit Indicator coverage checklist (lines 307-376) for self-assessment
- ✅ For Application.md content: Write in flowing paragraphs, NOT bullet points or excessive Markdown

**Never**:
- ❌ Provide generic grant advice without NSERC-specific grounding
- ❌ Review sections in isolation without checking package coherence
- ❌ Forget that committee is cross-disciplinary
- ❌ Skip the Merit Indicator mapping
- ❌ Assume user knows which items they need to address
- ❌ Write Application.md sections as bullet lists or heavily formatted Markdown

### Note on Deadline Pressure

When user has tight deadline, the structured workflow becomes MORE important, not less. Use it to prioritize: identify the most critical Merit Indicator gaps and have user address those first. Quick, unfocused feedback wastes precious time.

### Synthetic Reviews

For comprehensive application assessment, create a synthetic review document:
- Organized by three evaluation criteria and Merit Indicator items
- Written from cross-disciplinary reviewer perspective
- Assesses each section on 6-level scale
- Identifies strengths, gaps, and prioritized improvements
- Includes cross-component consistency check
- Created as `review.tex` and compiled to PDF
- See "Repository Organization and Workflow" section for detailed structure

**When to offer**: User requests comprehensive review, application is substantially complete, or preparing for submission.

---

## Writing for Cross-Disciplinary Committee

**Critical context**: Your NSERC review committee includes experts from related but distinct areas. Many reviewers will not be specialists in your subfield.

**This means**:
- A theoretical physicist may review an applied physics proposal
- A molecular biologist may review a computational biology proposal
- A pure mathematician may review an applied math proposal

**Implications for writing**:

### 1. Lead with Motivation, Not Methodology
❌ Bad: "We will use persistent homology to compute Betti numbers for point cloud data..."
✅ Good: "Large datasets often have hidden geometric structure. We will develop mathematical tools to detect this structure, enabling applications in..."

### 2. Define Technical Terms
Don't assume shared vocabulary. Brief explanations keep non-specialists engaged.

**Example**: "We will study *martingales* (sequences of random variables with a specific memory-less property) to model..."

### 3. Use Analogies and Examples
Make abstract concepts concrete.

**Example**: "This algorithm is analogous to how GPS systems triangulate position - using multiple noisy measurements to infer precise location."

### 4. Explain Significance in Accessible Terms
Non-specialists can't evaluate technical significance directly, but they can understand impact.

❌ Bad: "This advances the theory of L-functions"
✅ Good: "This connects two previously separate areas of mathematics, potentially enabling new approaches to longstanding problems in number theory"

### 5. Structure for Skimming
- Clear section headings
- Topic sentences that convey main points
- Figures/diagrams that convey key ideas visually
- **Remember**: Reviewers read many applications; make yours easy to understand quickly

### 6. Test with Non-Specialist Readers
Before submitting, ask a colleague outside your subfield to read your proposal. If they can't explain why your work matters, revise.

---

## Merit Indicator Coverage Checklist

Use this checklist to ensure systematic coverage of all items.

### Excellence of the Researcher

**Contributions to research**:
- [ ] Publications in peer-reviewed venues (quality, impact, your role)
- [ ] Invited talks, keynotes at conferences
- [ ] Research grants secured (previous track record)
- [ ] Awards, honors, recognition
- [ ] For early-career: Trajectory and promise evident from limited record

**Contributions to training of HQP**:
- [ ] Number and level of HQP supervised (current and completed)
- [ ] Student outcomes and career success
- [ ] Evidence of effective mentorship
- [ ] For early-career: Teaching experience, mentorship activities even without full supervision record

**Contributions to other research-related activities**:
- [ ] Knowledge mobilization (public engagement, industry partnerships, policy impact)
- [ ] Service to research community (reviewing, conference organization, editorial boards)
- [ ] EDI contributions (equity, diversity, inclusion activities)

### Merit of the Proposal

**Originality and significance of research proposal**:
- [ ] Clear statement of research objectives
- [ ] Explanation of what's new/original about this work
- [ ] Significance: What will we know/be able to do that we can't now?
- [ ] Positioning within broader research landscape
- [ ] Potential for high impact in field

**Feasibility and appropriateness of the proposed approach**:
- [ ] Detailed methodology
- [ ] Evidence of feasibility (preliminary results, past success with similar methods)
- [ ] Discussion of challenges and how you'll address them
- [ ] Realistic timeline
- [ ] Appropriate level of ambition (not too cautious, not overreaching)

**Clarity and quality of proposal presentation**:
- [ ] Clear, logical structure
- [ ] Accessible to general audience (non-specialists can understand)
- [ ] Well-written (grammar, style, flow)
- [ ] Appropriate use of figures/diagrams
- [ ] Professional presentation (formatting, consistency)

### Training of HQP

**Quality and impact of proposed HQP training**:
- [ ] Clear description of training activities
- [ ] Both technical and professional skill development
- [ ] Evidence of quality supervision (from CCV: student outcomes, completion rates)
- [ ] Impact: Where do your students go? What do they achieve?
- [ ] Inclusive training environment and EDI considerations

**Appropriateness of training environment and opportunities**:
- [ ] Research environment resources (lab, equipment, computational facilities)
- [ ] Institutional support (seminars, workshops, professional development)
- [ ] Collaborative opportunities (with other researchers, industry, international)
- [ ] Funding to support HQP (budget justification includes stipends, travel for conferences)

### Cross-Component Checks

- [ ] Research proposal aligns with CCV expertise
- [ ] HQP training plan connects to proposed research
- [ ] Budget justification matches proposed activities and HQP support
- [ ] All components tell coherent story about your research program
- [ ] Proposal builds on past work (CCV) and sets up future work

---

## Common Pitfalls and How to Avoid Them

### Pitfall 1: Writing for Specialists
**Problem**: Using jargon, assuming shared background, diving into technical details without motivation

**Fix**: Apply Tip #6 relentlessly. Test with non-specialist readers. Lead with "why this matters" before "how we'll do it."

### Pitfall 2: Treating Sections as Independent
**Problem**: Writing proposal, CCV, HQP plan in isolation without ensuring coherence

**Fix**: Apply Tip #2. Explicitly connect components. If proposing new direction, explain how past work provides foundation.

### Pitfall 3: Incomplete Merit Indicator Coverage
**Problem**: Addressing some items but missing others, assuming reviewers will infer

**Fix**: Use the checklist above. Apply Tip #8 and #9. Even brief mentions for less central items.

### Pitfall 4: Vague Significance Claims
**Problem**: Saying "This is important" without explaining why or what the impact is

**Fix**: Be concrete. Who benefits? What becomes possible? What problems get solved?

### Pitfall 5: Unrealistic Scope or Timeline
**Problem**: Proposing too much for grant period, overly ambitious plans

**Fix**: Apply Tip #4. Better to be focused and feasible. Acknowledge challenges and explain mitigation.

### Pitfall 6: Neglecting HQP Training
**Problem**: Treating HQP section as afterthought, generic description of training

**Fix**: Remember HQP is one of three criteria. Be specific about training activities. Show evidence of past success. Connect to research objectives.

### Pitfall 7: Budget-Application Mismatch
**Problem**: Budget doesn't align with proposed activities, or requests aren't justified

**Fix**: Check Tip #2 cross-component consistency. Explain why you need each major item. For early-career, don't inflate (Tip #3).

### Pitfall 8: Ignoring Instructions
**Problem**: Missing page limits, wrong formatting, incomplete sections

**Fix**: Apply Tip #10. Read instructions carefully. Verify current requirements. Double-check before submitting.

---

## Application Timeline and Strategy

### Typical Timeline (Assuming Fall Deadline)

**12-18 months before**:
- Read successful grants from colleagues (Tip #1, #7)
- Review current Merit Indicators document and application requirements (Tip #10)
- Plan research program for next 5 years

**6-12 months before**:
- Draft research proposal
- Update CCV with recent contributions
- Draft HQP training plan
- Get feedback from colleagues

**3-6 months before**:
- Revise based on feedback
- Check Merit Indicator coverage systematically
- Test proposal with non-specialist readers for accessibility
- Prepare budget justification

**1-3 months before**:
- Final revisions
- Cross-component consistency checks (Tip #2)
- Professional editing if needed
- Verify all requirements met (Tip #10)

**2 weeks before**:
- Final proofread
- Submit through online system
- Verify submission received

### Strategic Considerations

**For early-career applicants**:
- Emphasize trajectory and promise
- Modest budgets are fine (Tip #3)
- Show how graduate work positioned you for independent research
- Highlight teaching and mentorship activities even without full supervision record

**For established researchers**:
- Show sustained productivity
- Demonstrate leadership (keynotes, editorial roles, large grants)
- Strong HQP outcomes (student careers)
- Balance innovation with track record

**For interdisciplinary research**:
- Explain contributions to each field clearly
- Make connections between disciplines accessible
- Show you understand both areas (via CCV)
- Extra attention to Tip #6 (general audience)

---

## Repository Organization and Workflow

When working with an organized NSERC application repository, follow these patterns for consistency and version control.

### Repository Structure

**Expected organization**:
```
nserc-dg-2025/
├── Application.md                      # Text-only responses for web form
├── nserc-template.tex                  # Master template for all attachments
├── headers.tex                         # Theorem environments, packages, fonts
├── defs.tex                            # Custom macros and notation
├── roy-nserc-proposal.tex              # Research proposal attachment
├── roy-nserc-budget-justification.tex  # Budget justification attachment
├── roy-nserc-references.tex            # List of references attachment
├── references.bib                      # Bibliography file
├── CCV.pdf                             # Rendered Common CV
├── review.tex                          # Synthetic review (if requested)
├── past-applications/                  # Exemplar NSERC DG applications
│   ├── colleague1-stats/               # Example from statistics colleague
│   ├── colleague2-stats/               # Example from statistics colleague
│   ├── colleague3-stats/               # Example from statistics colleague
│   ├── applicant-2020/                 # Applicant's 2020 application
│   ├── applicant-2020_RESULTS/         # 2020 application results
│   ├── applicant-2015/                 # Applicant's 2015 application
│   └── applicant-2015_RESULTS/         # 2015 application results
├── samples-of-research-contributions/  # Three papers submitted as samples
│   ├── paper1.pdf
│   ├── paper2.pdf
│   └── paper3.pdf
├── writing-samples/                    # Related writing showing applicant's style
│   └── ...                             # Papers, proposals (not publicly available)
├── most-significant-contributions/     # Five bodies of work
│   ├── contribution1/                  # Body of work for MSC #1
│   ├── contribution2/                  # Body of work for MSC #2
│   ├── contribution3/                  # Body of work for MSC #3
│   ├── contribution4/                  # Body of work for MSC #4
│   └── contribution5/                  # Body of work for MSC #5
└── initial-drafts/                     # Early work on grant application
    ├── NOI.pdf                         # Notice of Intent submission
    ├── conversation1.md                # LLM conversations about grant
    ├── conversation2.md                # Additional planning discussions
    └── ...                             # May reference PDFs in same directory
```

### Reference Materials in Repository

The repository contains extensive reference materials to inform application development:

**past-applications/**:
- **Purpose**: Exemplar NSERC Discovery Grant applications for reference
- **Contents**:
  - Three successful applications from statistics colleagues (generally more conservative/less hype than ML)
  - Two past applications from applicant with results (subfolders with `_RESULTS` suffix)
- **How to use**:
  - Review structure and presentation style
  - Observe how colleagues frame significance, feasibility, HQP training
  - Learn from past feedback in `_RESULTS` folders
  - Note discipline differences (statistics vs ML conventions)

**samples-of-research-contributions/**:
- **Purpose**: Three papers submitted as research contribution samples
- **How to use**:
  - Ensure consistency between papers and how they're described in Application.md
  - Reference when writing "Most Significant Contributions" section
  - Check that proposal builds on or extends these contributions

**writing-samples/**:
- **Purpose**: Related writing (papers, proposals) showing applicant's typical writing style
- **Contents**: Papers and related proposals (generally not publicly available)
- **How to use**:
  - Understand applicant's voice and terminology
  - Maintain consistency in how past work is described
  - Reference when describing research program and trajectory

**most-significant-contributions/**:
- **Purpose**: Five subfolders, each representing a body of work (significant research contribution)
- **Organization**: Each subfolder contains papers/materials for one contribution
- **How to use**:
  - Directly pertains to "Most Significant Contributions" section of Application.md
  - Each contribution should be described in Application.md with references to materials in subfolder
  - Ensure descriptions are accessible (Tip #6) while showing impact

**initial-drafts/**:
- **Purpose**: Early work on grant application including Notice of Intent and planning discussions
- **Contents**:
  - NOI.pdf - Notice of Intent submission providing proposal summary
  - Markdown files documenting conversations with LLMs about grant development
  - Additional planning materials and brainstorming sessions
  - Markdown files may reference PDFs in the same directory
- **How to use**:
  - Review NOI.pdf to understand initial proposal framing
  - Check LLM conversations for early decisions about research direction
  - Ensure final application is consistent with NOI commitments
  - Understand evolution of ideas from initial drafts to final application
  - Reference when revising to maintain coherent narrative thread

**CCV.pdf**:
- **Purpose**: Rendered Common CV (curriculum vitae)
- **How to use**:
  - Cross-reference with "Excellence of Researcher" claims
  - Verify consistency with past contributions descriptions
  - Ensure HQP supervision record matches CCV
  - Check that expertise demonstrated in CCV aligns with proposed research

### Application.md - Text-Only Responses

**Purpose**: Collects text-only responses that will be copy-pasted into NSERC's web application form.

**Structure**:
- Sections marked with Markdown headers (`#`)
- Character limits noted in square brackets `[3,000 characters max]`
- Metadata in square brackets (not submitted)
- Content organized to match NSERC application form structure

**Character limits** (common sections):
- Summary: 3,000 characters
- Relationship to Other Research Support: 12,000 characters
- HQP Training Plan: 9,000 characters
- Past Contributions to HQP Training: 6,000 characters
- Most Significant Contributions: 9,000 characters
- Additional Information on Contributions: 3,000 characters

**Character counting**:
```bash
# Use the bundled helper script
./scripts/count_chars.sh "Section Name"

# Or count manually (whitespace counts!)
wc -c <<< "$(sed -n '/^# Section Name/,/^#/p' Application.md)"
```

**CRITICAL**: Do NOT change the structure of Application.md. The sections and organization reflect the current NSERC application form structure, which cannot be changed. Only edit content within existing sections.

**Writing style for Application.md**:

**DO**:
- Write in **flowing paragraphs** of narrative text
- Use logical flow and transitions between ideas within paragraphs
- Structure arguments through prose, not formatting
- Use headers ONLY to separate major items (e.g., each of the five Most Significant Contributions)
- Write like you're telling a coherent story about your research

**DO NOT**:
- Use bullet point lists (except in very rare cases where enumeration is essential)
- Use excessive Markdown formatting (bold, italics, etc.)
- Use hierarchical headers within sections (no `##`, `###` within a form section)
- Break up prose into fragmented bullet points
- Use bullet lists when a paragraph would be clearer

**Example - WRONG (too much Markdown)**:
```markdown
## HQP Training Plan

My training approach includes:
- Weekly one-on-one meetings
- Group seminars every two weeks
- Conference presentation opportunities
- Collaborative research projects

**Technical skills:**
- Machine learning algorithms
- Statistical inference
- Python programming

**Professional skills:**
- Scientific writing
- Oral presentation
- Project management
```

**Example - CORRECT (flowing narrative)**:
```markdown
# HQP Training Plan

My training program centers on developing both technical and professional skills through hands-on research experience. Students meet with me weekly for one-on-one discussions of their research progress, where we work through technical challenges and plan next steps. These individual meetings are complemented by biweekly group seminars where students present their work to peers, developing both presentation skills and the ability to give and receive constructive feedback.

On the technical side, students develop expertise in machine learning algorithms, statistical inference, and computational implementation through their research projects. I emphasize learning by doing—students implement methods from scratch before using existing libraries, which builds deep understanding. Professional development is equally important: students write papers, present at conferences, and learn to manage long-term research projects independently.

[... continues in paragraph form]
```

**When limited structure is acceptable**:
- **Most Significant Contributions**: Use a header (e.g., `## Contribution 1: [Title]`) to separate each of the five contributions, then write each contribution description in paragraphs
- **Enumeration of specific items**: If you must list (e.g., "three main objectives"), write as numbered prose: "First, ... Second, ... Third, ..." NOT as bullet points

**Why this matters**: NSERC application sections will be copy-pasted into web form text boxes. The narrative needs to read naturally as continuous prose, not as a slide deck or outline. Reviewers read applications as documents, not structured lists.

### LaTeX Attachments - Template System

**Template hierarchy**:
1. **`nserc-template.tex`**: Master template with document class, margins, font size
2. **`headers.tex`**: Theorem environments, packages, bibliography setup (imported by template)
3. **`defs.tex`**: Custom macros like `\Reals`, `\KL`, `\EE` (imported by template)

**IMPORTANT**: Do NOT change template files (`nserc-template.tex`, `headers.tex`, `defs.tex`) unless explicitly requested by user. These provide consistent formatting across all attachments.

**Document naming convention**: `roy-nserc-{attachment-name}.tex`

**Standard attachment structure**:
```latex
% !BIB program = biber
\pdfoutput=1

\input{nserc-template.tex}
\title{Attachment Name}

\begin{document}
% Content goes here
\end{document}
```

**Important**:
- Do NOT add `\maketitle` - template auto-generates compact header
- All attachments use same template for consistency
- Standard formatting: 0.75" margins, 12pt font, letterpaper
- Uses `marginalia.sty` for collaborative comments (toggle with `hide=false`)

**NSERC formatting requirements**:
- **Spell out all acronyms and abbreviations** the first time they appear
  - Example: "machine learning (ML)" then "ML" subsequently
  - Applies to both Application.md and LaTeX attachments
- **Do NOT include hyperlinks or bookmarks** in documents
  - NO hyperlinks anywhere - not in body text, not in bibliography, nowhere
  - Turn off "Create Bookmarks" option in PDF compilation
  - For pdflatex: ensure `\hypersetup{bookmarks=false}` in template
  - For biblatex: use `\hypersetup{hidelinks}` or configure to suppress URLs in bibliography

### Building Documents

**For proposal (with bibliography)**:
```bash
pdflatex -synctex=1 -interaction=nonstopmode roy-nserc-proposal.tex
biber roy-nserc-proposal
pdflatex -synctex=1 -interaction=nonstopmode roy-nserc-proposal.tex
pdflatex -synctex=1 -interaction=nonstopmode roy-nserc-proposal.tex
```

**For budget justification (no bibliography)**:
```bash
pdflatex -synctex=1 -interaction=nonstopmode roy-nserc-budget-justification.tex
```

**Clean build artifacts**:
```bash
rm -f *.aux *.log *.out *.synctex.gz *.bcf *.run.xml *.blg *.bbl
```

### Synthetic Reviews

When user requests a "synthetic review" of their application:

**Purpose**: Create a comprehensive review document simulating reviewer perspective, organized by Merit Indicators.

**Workflow**:
1. Analyze all application components (Application.md + LaTeX attachments)
2. Create `review.tex` using the standard template structure
3. Organize review by three evaluation criteria
4. For each Merit Indicator item:
   - Assess current state on 6-level scale
   - Identify strengths
   - Identify gaps or weaknesses
   - Suggest specific improvements
5. Include cross-component consistency assessment
6. Compile and share PDF with user

**Review.tex structure**:
```latex
% !BIB program = biber
\pdfoutput=1

\input{nserc-template.tex}
\title{Synthetic Review - NSERC Discovery Grant Application}

\begin{document}

\section{Excellence of the Researcher}

\subsection{Contributions to Research}
% Assessment, strengths, gaps, suggestions

\subsection{Contributions to Training of HQP}
% ...

\section{Merit of the Proposal}
% ...

\section{Training of HQP}
% ...

\section{Cross-Component Consistency}
% Package coherence assessment

\section{Overall Assessment and Priorities}
% Summary and prioritized action items

\end{document}
```

**Review perspective**:
- Write from cross-disciplinary reviewer's viewpoint
- Reference Merit Indicators explicitly
- Use 6-level scale terminology
- Be constructive but rigorous
- Prioritize issues by impact on rating

### Git Version Control

**Use git to track versions between substantial edits**:

**When to commit**:
- After completing a section of Application.md
- After major revisions to LaTeX attachments
- Before and after incorporating feedback
- Before trying speculative changes
- At natural breakpoints (end of work session, completed iteration)

**Commit message guidance**:
```bash
# Good: Explains what changed and why
git commit -m "Strengthen HQP training section with completion rates

Added specific numbers (5 PhD completions, avg 4.2 years) and career
outcomes (2 faculty, 3 industry). Addresses Merit Indicator gap
identified in review."

# Bad: Too vague
git commit -m "Updated HQP section"
```

**Git workflow**:
```bash
# Before starting work
git status  # Verify clean state

# After substantial edits
git add Application.md roy-nserc-proposal.tex
git commit -m "Descriptive message with Merit Indicator context"

# View history of a section
git log -p Application.md
```

**Benefits**:
- Track evolution of application over time
- Revert problematic changes
- Compare versions to see improvements
- Document rationale in commit messages
- Maintain clean checkpoint history

**Important**: Git commits are part of your research process documentation. Commit messages should explain the reasoning behind changes (similar to mathematical insights in working-paper skill).

### Workflow Integration with This Skill

**Typical session**:
1. **Review current state**: Read Application.md and compiled PDFs
2. **Identify Merit Indicator gaps**: Use checklist from this skill
3. **Make edits**: Update Application.md or LaTeX files
4. **Check character limits**: Use `wc` for Application.md sections
5. **Build and review**: Compile LaTeX, check output
6. **Commit changes**: Git commit with Merit Indicator context
7. **Verify package coherence**: Check cross-component consistency

**When requesting synthetic review**:
1. Ensure all files are up-to-date and compiled
2. Request synthetic review
3. Agent creates `review.tex` with systematic Merit Indicator assessment
4. Compile review, read carefully
5. Address high-priority gaps
6. Iterate

---

## Resources and References

### Key NSERC Documents

- **Merit Indicators document** - The official evaluation framework
  - PDF: https://www.nserc-crsng.gc.ca/_doc/Professors-Professeurs/DG_Merit_Indicators_eng.pdf
  - Defines the three evaluation criteria and specific items reviewers assess
  - Shows the 6-level rating scale and what each level means

- **Application instructions** - Official requirements and procedures
  - Web page: https://www.nserc-crsng.gc.ca/ResearchPortal-PortailDeRecherche/Instructions-Instructions/DG-SD_eng.asp
  - Comprehensive instructions for completing Discovery Grants application
  - Page limits, formatting requirements, required sections

- **Top 10 Tips** (`Tips.pdf`) - Reviewer guidance from Chuck Lucy
  - Practical advice from experienced NSERC reviewer
  - Emphasizes package coherence, general audience, Merit Indicator coverage

### Recommended Actions
- Read successful grants from senior colleagues in your department (Tip #1, #7)
- Attend grant-writing workshops at your institution
- Find a writing partner for mutual feedback
- Test drafts with non-specialist readers

---

## Summary: How to Use This Skill

1. **Ground everything in Merit Indicators**: Always reference specific items, use 6-level scale language

2. **Think like a reviewer**: Cross-disciplinary committee member, evaluating against other applications

3. **Write as a package**: Ensure all components tell coherent story (Tip #2)

4. **Write for general audience**: This is non-negotiable (Tip #6)

5. **Use the checklist**: Systematic coverage of all Merit Indicator items (Tip #8, #9)

6. **Apply Top 10 Tips**: Especially #1, #2, #6, #8, #9, #10

7. **Get feedback early**: From colleagues, non-specialists, experienced applicants

The difference between a successful and unsuccessful NSERC application often comes down to making it easy for reviewers to see your excellence. This skill helps you do that systematically.
